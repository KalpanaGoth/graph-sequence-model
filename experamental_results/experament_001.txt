We ran the Training....
Run python run_experiments.py
Configuration loaded successfully: {'experiment_name': 'graph_based_model_experiment', 'random_seed': 42, 'device': 'cuda', 'data': {'train_data_path': 'sample_datasets/train_data.csv', 'val_data_path': 'sample_datasets/val_data.csv', 'test_data_path': 'sample_datasets/test_data.csv', 'batch_size': 32, 'num_workers': 4}, 'model': {'input_dim': 10, 'hidden_dim': 64, 'output_dim': 1, 'use_attention': True}, 'training': {'num_epochs': 100, 'learning_rate': 0.001, 'weight_decay': '1e-5', 'patience': 10, 'step_size': 10, 'gamma': 0.1}, 'output': {'checkpoints_dir': 'outputs/checkpoints/', 'logs_dir': 'outputs/logs/', 'results_dir': 'outputs/results/'}}
/Users/jose.sosa/Documents/git/graph-sequence-model/venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Epoch 1/100 - Loss: 0.7622408866882324
Epoch 2/100 - Loss: 0.5636951327323914
Epoch 3/100 - Loss: 0.417441189289093
Epoch 4/100 - Loss: 0.3192538321018219
Epoch 5/100 - Loss: 0.2644387185573578
Epoch 6/100 - Loss: 0.2471432238817215
Epoch 7/100 - Loss: 0.25826412439346313
Epoch 8/100 - Loss: 0.28348636627197266
Epoch 9/100 - Loss: 0.3067184090614319
Epoch 10/100 - Loss: 0.318130224943161
Epoch 11/100 - Loss: 0.3163182735443115
Epoch 12/100 - Loss: 0.3151264488697052
Epoch 13/100 - Loss: 0.3131527900695801
Epoch 14/100 - Loss: 0.310577929019928
Epoch 15/100 - Loss: 0.30755293369293213
Epoch 16/100 - Loss: 0.3042047917842865
Epoch 17/100 - Loss: 0.3006400167942047
Epoch 18/100 - Loss: 0.29694756865501404
Epoch 19/100 - Loss: 0.29320162534713745
Epoch 20/100 - Loss: 0.28946369886398315
Epoch 21/100 - Loss: 0.28578421473503113
Epoch 22/100 - Loss: 0.2854176461696625
Epoch 23/100 - Loss: 0.2850452661514282
Epoch 24/100 - Loss: 0.2846679389476776
Epoch 25/100 - Loss: 0.28428640961647034
Epoch 26/100 - Loss: 0.2839013934135437
Epoch 27/100 - Loss: 0.28351351618766785
Epoch 28/100 - Loss: 0.2831234335899353
Epoch 29/100 - Loss: 0.2827315926551819
Epoch 30/100 - Loss: 0.2823385000228882
Epoch 31/100 - Loss: 0.28194451332092285
Epoch 32/100 - Loss: 0.2819049656391144
Epoch 33/100 - Loss: 0.2818652093410492
Epoch 34/100 - Loss: 0.28182512521743774
Epoch 35/100 - Loss: 0.28178489208221436
Epoch 36/100 - Loss: 0.2817443907260895
Epoch 37/100 - Loss: 0.28170374035835266
Epoch 38/100 - Loss: 0.28166288137435913
Epoch 39/100 - Loss: 0.2816218137741089
Epoch 40/100 - Loss: 0.2815805673599243
Epoch 41/100 - Loss: 0.2815392017364502
Epoch 42/100 - Loss: 0.2815350592136383
Epoch 43/100 - Loss: 0.28153085708618164
Epoch 44/100 - Loss: 0.28152668476104736
Epoch 45/100 - Loss: 0.2815224826335907
Epoch 46/100 - Loss: 0.28151825070381165
Epoch 47/100 - Loss: 0.2815140187740326
Epoch 48/100 - Loss: 0.28150975704193115
Epoch 49/100 - Loss: 0.2815054655075073
Epoch 50/100 - Loss: 0.2815012037754059
Epoch 51/100 - Loss: 0.28149688243865967
Epoch 52/100 - Loss: 0.28149646520614624
Epoch 53/100 - Loss: 0.2814960181713104
Epoch 54/100 - Loss: 0.281495600938797
Epoch 55/100 - Loss: 0.28149518370628357
Epoch 56/100 - Loss: 0.28149479627609253
Epoch 57/100 - Loss: 0.2814943194389343
Epoch 58/100 - Loss: 0.2814939022064209
Epoch 59/100 - Loss: 0.28149348497390747
Epoch 60/100 - Loss: 0.28149300813674927
Epoch 61/100 - Loss: 0.28149259090423584
Epoch 62/100 - Loss: 0.28149259090423584
Epoch 63/100 - Loss: 0.2814926207065582
Epoch 64/100 - Loss: 0.28149256110191345
Epoch 65/100 - Loss: 0.28149259090423584
Epoch 66/100 - Loss: 0.28149259090423584
Epoch 67/100 - Loss: 0.28149256110191345
Epoch 68/100 - Loss: 0.28149259090423584
Epoch 69/100 - Loss: 0.28149256110191345
Epoch 70/100 - Loss: 0.28149253129959106
Epoch 71/100 - Loss: 0.28149253129959106
Epoch 72/100 - Loss: 0.28149253129959106
Epoch 73/100 - Loss: 0.28149253129959106
Epoch 74/100 - Loss: 0.28149253129959106
Epoch 75/100 - Loss: 0.2814925014972687
Epoch 76/100 - Loss: 0.28149253129959106
Epoch 77/100 - Loss: 0.28149253129959106
Epoch 78/100 - Loss: 0.28149253129959106
Epoch 79/100 - Loss: 0.2814925014972687
Epoch 80/100 - Loss: 0.28149253129959106
Epoch 81/100 - Loss: 0.28149253129959106
Epoch 82/100 - Loss: 0.28149253129959106
Epoch 83/100 - Loss: 0.28149253129959106
Epoch 84/100 - Loss: 0.28149253129959106
Epoch 85/100 - Loss: 0.28149253129959106
Epoch 86/100 - Loss: 0.28149256110191345
Epoch 87/100 - Loss: 0.28149253129959106
Epoch 88/100 - Loss: 0.28149253129959106
Epoch 89/100 - Loss: 0.28149253129959106
Epoch 90/100 - Loss: 0.28149253129959106
Epoch 91/100 - Loss: 0.28149253129959106
Epoch 92/100 - Loss: 0.2814925014972687
Epoch 93/100 - Loss: 0.28149253129959106
Epoch 94/100 - Loss: 0.28149256110191345
Epoch 95/100 - Loss: 0.28149253129959106
Epoch 96/100 - Loss: 0.28149253129959106
Epoch 97/100 - Loss: 0.28149253129959106
Epoch 98/100 - Loss: 0.28149253129959106
Epoch 99/100 - Loss: 0.28149256110191345
Epoch 100/100 - Loss: 0.28149253129959106
Traceback (most recent call last):
  File "/Users/jose.sosa/Documents/git/graph-sequence-model/run_experiments.py", line 111, in <module>
    main()
  File "/Users/jose.sosa/Documents/git/graph-sequence-model/run_experiments.py", line 107, in main
    metrics = evaluate_model(model, dataloader, criterion)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jose.sosa/Documents/git/graph-sequence-model/evaluate/evaluate.py", line 59, in evaluate_model
    inputs, labels = batch
    ^^^^^^^^^^^^^^
ValueError: too many values to unpack (expected 2)
(venv) (base) ➜  graph-sequence-model git:(main) ✗ python run_experiments.py
Configuration loaded successfully: {'experiment_name': 'graph_based_model_experiment', 'random_seed': 42, 'device': 'cuda', 'data': {'train_data_path': 'sample_datasets/train_data.csv', 'val_data_path': 'sample_datasets/val_data.csv', 'test_data_path': 'sample_datasets/test_data.csv', 'batch_size': 32, 'num_workers': 4}, 'model': {'input_dim': 10, 'hidden_dim': 64, 'output_dim': 1, 'use_attention': True}, 'training': {'num_epochs': 100, 'learning_rate': 0.001, 'weight_decay': '1e-5', 'patience': 10, 'step_size': 10, 'gamma': 0.1}, 'output': {'checkpoints_dir': 'outputs/checkpoints/', 'logs_dir': 'outputs/logs/', 'results_dir': 'outputs/results/'}}
/Users/jose.sosa/Documents/git/graph-sequence-model/venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Epoch 1/100 - Loss: 0.8251664042472839
Epoch 2/100 - Loss: 0.5863116979598999
Epoch 3/100 - Loss: 0.41635653376579285
Epoch 4/100 - Loss: 0.3100612759590149
Epoch 5/100 - Loss: 0.2605683207511902
Epoch 6/100 - Loss: 0.25757813453674316
Epoch 7/100 - Loss: 0.2840421795845032
Epoch 8/100 - Loss: 0.3169756829738617
Epoch 9/100 - Loss: 0.33784201741218567
Epoch 10/100 - Loss: 0.34040236473083496
Epoch 11/100 - Loss: 0.32794058322906494
Epoch 12/100 - Loss: 0.3257320523262024
Epoch 13/100 - Loss: 0.3227533996105194
Epoch 14/100 - Loss: 0.31919851899147034
Epoch 15/100 - Loss: 0.3152307868003845
Epoch 16/100 - Loss: 0.3109871447086334
Epoch 17/100 - Loss: 0.30658259987831116
Epoch 18/100 - Loss: 0.3021128177642822
Epoch 19/100 - Loss: 0.2976570129394531
Epoch 20/100 - Loss: 0.29328006505966187
Epoch 21/100 - Loss: 0.28903454542160034
Epoch 22/100 - Loss: 0.2886162996292114
Epoch 23/100 - Loss: 0.28819432854652405
Epoch 24/100 - Loss: 0.28776928782463074
Epoch 25/100 - Loss: 0.28734177350997925
Epoch 26/100 - Loss: 0.2869124710559845
Epoch 27/100 - Loss: 0.28648170828819275
Epoch 28/100 - Loss: 0.2860499918460846
Epoch 29/100 - Loss: 0.28561779856681824
Epoch 30/100 - Loss: 0.28518542647361755
Epoch 31/100 - Loss: 0.28475335240364075
Epoch 32/100 - Loss: 0.2847100496292114
Epoch 33/100 - Loss: 0.2846665680408478
Epoch 34/100 - Loss: 0.28462284803390503
Epoch 35/100 - Loss: 0.28457894921302795
Epoch 36/100 - Loss: 0.28453490138053894
Epoch 37/100 - Loss: 0.2844906449317932
Epoch 38/100 - Loss: 0.28444620966911316
Epoch 39/100 - Loss: 0.2844015657901764
Epoch 40/100 - Loss: 0.2843567728996277
Epoch 41/100 - Loss: 0.28431186079978943
Epoch 42/100 - Loss: 0.2843073308467865
Epoch 43/100 - Loss: 0.28430280089378357
Epoch 44/100 - Loss: 0.28429824113845825
Epoch 45/100 - Loss: 0.2842937409877777
Epoch 46/100 - Loss: 0.2842891216278076
Epoch 47/100 - Loss: 0.2842845320701599
Epoch 48/100 - Loss: 0.2842799127101898
Epoch 49/100 - Loss: 0.28427526354789734
Epoch 50/100 - Loss: 0.28427064418792725
Epoch 51/100 - Loss: 0.28426593542099
Epoch 52/100 - Loss: 0.2842654585838318
Epoch 53/100 - Loss: 0.2842649817466736
Epoch 54/100 - Loss: 0.28426453471183777
Epoch 55/100 - Loss: 0.2842640280723572
Epoch 56/100 - Loss: 0.2842635214328766
Epoch 57/100 - Loss: 0.28426310420036316
Epoch 58/100 - Loss: 0.28426259756088257
Epoch 59/100 - Loss: 0.28426215052604675
Epoch 60/100 - Loss: 0.28426167368888855
Epoch 61/100 - Loss: 0.28426116704940796
Epoch 62/100 - Loss: 0.28426116704940796
Epoch 63/100 - Loss: 0.28426116704940796
Epoch 64/100 - Loss: 0.28426116704940796
Epoch 65/100 - Loss: 0.28426113724708557
Epoch 66/100 - Loss: 0.28426116704940796
Epoch 67/100 - Loss: 0.28426113724708557
Epoch 68/100 - Loss: 0.28426113724708557
Epoch 69/100 - Loss: 0.28426113724708557
Epoch 70/100 - Loss: 0.2842611074447632
Epoch 71/100 - Loss: 0.28426113724708557
Epoch 72/100 - Loss: 0.2842611074447632
Epoch 73/100 - Loss: 0.2842611074447632
Epoch 74/100 - Loss: 0.2842611074447632
Epoch 75/100 - Loss: 0.2842611074447632
Epoch 76/100 - Loss: 0.2842610776424408
Epoch 77/100 - Loss: 0.2842611074447632
Epoch 78/100 - Loss: 0.2842611074447632
Epoch 79/100 - Loss: 0.2842611074447632
Epoch 80/100 - Loss: 0.2842611074447632
Epoch 81/100 - Loss: 0.2842611074447632
Epoch 82/100 - Loss: 0.2842611074447632
Epoch 83/100 - Loss: 0.2842611074447632
Epoch 84/100 - Loss: 0.2842611074447632
Epoch 85/100 - Loss: 0.2842611074447632
Epoch 86/100 - Loss: 0.2842611074447632
Epoch 87/100 - Loss: 0.2842611074447632
Epoch 88/100 - Loss: 0.2842611074447632
Epoch 89/100 - Loss: 0.2842611074447632
Epoch 90/100 - Loss: 0.2842610776424408
Epoch 91/100 - Loss: 0.2842611074447632
Epoch 92/100 - Loss: 0.2842611074447632
Epoch 93/100 - Loss: 0.2842611074447632
Epoch 94/100 - Loss: 0.2842611074447632
Epoch 95/100 - Loss: 0.2842611074447632
Epoch 96/100 - Loss: 0.2842611074447632
Epoch 97/100 - Loss: 0.2842611074447632
Epoch 98/100 - Loss: 0.2842611074447632
Epoch 99/100 - Loss: 0.2842611074447632
Epoch 100/100 - Loss: 0.2842611074447632
Final Evaluation Metrics: {'average_loss': 0.28426113724708557}